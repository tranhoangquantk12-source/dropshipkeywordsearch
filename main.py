import os
import json
import gspread
import requests
import time
import sys
from oauth2client.service_account import ServiceAccountCredentials

# --- C·∫§U H√åNH ---
SPREADSHEET_ID = "1uvjEg0XtG_Q8jNjPVQ6FP_9cIvqxyur-I-PHNggUy5s"
SHEET_KW_NAME = "kw"
SHEET_PUB_NAME = "Publisher"
SHEET_ART_NAME = "Article"

# Blacklist Domains
EXCLUDE_DOMAINS = [
    "youtube.com", "shopify.com", "autods.com", "omnisend.com", 
    "reddit.com", "quora.com", "coursera.org", "classcentral.com", 
    "trueprofit.io", "beprofit.co", "facebook.com", "instagram.com", 
    "tiktok.com", "threads.com", "x.com", "cursa.app", 
    "coursesity.com", "scribd.com", "alison.com", "udemy.com"
]

def get_google_sheet_client():
    creds_json = os.environ.get("GCP_SA_KEY")
    if not creds_json:
        raise Exception("Kh√¥ng t√¨m th·∫•y bi·∫øn m√¥i tr∆∞·ªùng GCP_SA_KEY.")
    
    creds_dict = json.loads(creds_json)
    scope = ["https://spreadsheets.google.com/feeds", "https://www.googleapis.com/auth/drive"]
    creds = ServiceAccountCredentials.from_json_keyfile_dict(creds_dict, scope)
    client = gspread.authorize(creds)
    return client

def search_serper(query, api_key, num_results=10):
    """H√†m search core"""
    url = "https://google.serper.dev/search"
    payload = json.dumps({"q": query, "num": 30}) 
    headers = {'X-API-KEY': api_key, 'Content-Type': 'application/json'}

    try:
        response = requests.request("POST", url, headers=headers, data=payload)
        data = response.json()
        
        if "organic" not in data:
            return []

        clean_results = []
        count = 0
        for item in data["organic"]:
            link = item.get("link")
            if not link: continue
            
            domain = link.split("//")[-1].split("/")[0].lower()
            is_blocked = False
            for blocked in EXCLUDE_DOMAINS:
                if blocked in domain:
                    is_blocked = True
                    break
            
            if not is_blocked:
                clean_results.append(link)
                count += 1
            if count == num_results:
                break     
        return clean_results

    except Exception as e:
        print(f"L·ªói Serper API khi search '{query}': {e}")
        return []

def process_and_save(keywords, target_sheet_obj, api_key, flow_name):
    """
    Logic x·ª≠ l√Ω: Search -> Check tr√πng v·ªõi data c≈© -> Ghi URL m·ªõi
    """
    print(f"\nüöÄ B·∫ÆT ƒê·∫¶U {flow_name}...")
    print(f"-> S·ªë l∆∞·ª£ng keyword c·∫ßn ch·∫°y: {len(keywords)}")
    
    if not keywords:
        print("-> Kh√¥ng c√≥ keyword n√†o. Skip.")
        return

    # --- B∆Ø·ªöC 1: L·∫§Y D·ªÆ LI·ªÜU C≈® ƒê·ªÇ CHECK TR√ôNG ---
    try:
        # L·∫•y to√†n b·ªô c·ªôt A hi·ªán c√≥ trong sheet ƒë√≠ch
        existing_urls_list = target_sheet_obj.col_values(1)
        # ƒê∆∞a v√†o SET ƒë·ªÉ check cho nhanh (O(1))
        existing_urls_set = set(existing_urls_list)
        print(f"-> ƒê√£ load {len(existing_urls_set)} URL c≈© ƒë·ªÉ ƒë·ªëi chi·∫øu.")
    except Exception as e:
        print(f"Warning: Kh√¥ng ƒë·ªçc ƒë∆∞·ª£c d·ªØ li·ªáu c≈© (c√≥ th·ªÉ sheet r·ªóng): {e}")
        existing_urls_set = set()

    data_to_write = []
    
    # --- B∆Ø·ªöC 2: QU√âT V√Ä L·ªåC ---
    for kw in keywords:
        print(f"   Searching: {kw}")
        urls = search_serper(kw, api_key)
        
        new_urls_count_for_kw = 0
        
        for url in urls:
            # Check tr√πng: N·∫øu URL ch∆∞a t·ª´ng c√≥ trong Set th√¨ m·ªõi l·∫•y
            if url not in existing_urls_set:
                data_to_write.append([url])
                
                # Quan tr·ªçng: Th√™m ngay v√†o Set ƒë·ªÉ tr√°nh tr√πng l·∫∑p 
                # ngay trong ch√≠nh l·∫ßn ch·∫°y n√†y (n·∫øu 2 kw ra c√πng 1 url)
                existing_urls_set.add(url)
                new_urls_count_for_kw += 1
        
        # In ra log nh·∫π ƒë·ªÉ bi·∫øt keyword n√†y ki·∫øm ƒë∆∞·ª£c bao nhi√™u c√°i m·ªõi
        if new_urls_count_for_kw > 0:
            print(f"     -> Th√™m ƒë∆∞·ª£c {new_urls_count_for_kw} URL m·ªõi.")
            
        time.sleep(0.5) 

    # --- B∆Ø·ªöC 3: GHI D·ªÆ LI·ªÜU ---
    if data_to_write:
        print(f"-> ƒêang ghi t·ªïng c·ªông {len(data_to_write)} URL M·ªöI TINH v√†o sheet...")
        target_sheet_obj.append_rows(data_to_write)
        print(f"‚úÖ {flow_name}: HO√ÄN TH√ÄNH.")
    else:
        print(f"‚ö†Ô∏è {flow_name}: Kh√¥ng c√≥ URL n√†o m·ªõi (to√†n b·ªô ƒë√£ tr√πng ho·∫∑c kh√¥ng t√¨m th·∫•y).")

def main():
    print("--- STARTING DUAL FLOW JOB (WITH DEDUPLICATION) ---")
    
    try:
        # 1. Init Connections
        client = get_google_sheet_client()
        sh = client.open_by_key(SPREADSHEET_ID)
        
        kw_sheet = sh.worksheet(SHEET_KW_NAME)
        pub_sheet = sh.worksheet(SHEET_PUB_NAME)
        art_sheet = sh.worksheet(SHEET_ART_NAME) 
        
        serper_api_key = os.environ.get("SERPER_API_KEY")
        if not serper_api_key:
             raise Exception("Thi·∫øu SERPER_API_KEY")

        # 2. CHU·∫®N B·ªä D·ªÆ LI·ªÜU ƒê·∫¶U V√ÄO
        
        # --- LU·ªíNG 1: Keyword c·ªôt A (Article) ---
        raw_col_a = kw_sheet.col_values(1)[1:] 
        keywords_group_a = [k for k in raw_col_a if k.strip()]

        # --- LU·ªíNG 2: Keyword c·ªôt B (Publisher) ---
        raw_col_b = kw_sheet.col_values(2)[1:] 
        keywords_group_b = [k for k in raw_col_b if k.strip()]

        # 3. TH·ª∞C THI C√ÅC LU·ªíNG
        
        # Ch·∫°y lu·ªìng cho Article
        process_and_save(keywords_group_a, art_sheet, serper_api_key, flow_name="FLOW 1 [Article]")

        # Ch·∫°y lu·ªìng cho Publisher
        process_and_save(keywords_group_b, pub_sheet, serper_api_key, flow_name="FLOW 2 [Publisher]")

    except Exception as e:
        print(f"\n‚ùå L·ªñI H·ªÜ TH·ªêNG: {e}")
        sys.exit(1)

    print("\n--- JOB FINISHED SUCCESSFULLY ---")

if __name__ == "__main__":
    main()
